{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Statistical Inference: IV\n",
    "\n",
    "*N. Pol (2025)*\n",
    "\n",
    "Material in this lecture and notebook is based upon the lectures of A. Connolly's & Ž. Ivezić's \"Astrostatistics & Machine Learning\" class at the University of Washington (ASTR 598, https://github.com/dirac-institute/uw-astr598-w18). Also the \"Inference2\" lecture of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), Karen Leighly's [Bayesian Stats](http://seminar.ouml.org/lectures/bayesian-statistics/), and J. Bovy's mini-course on \"Statistics & Inference in Astrophysics\" at the University of Toronto (http://astro.utoronto.ca/~bovy/teaching.html). \n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 5.\n",
    "- [MCMC Sampling](https://twiecki.io/blog/2015/11/10/mcmc-sampling) by Thomas Wiecki.\n",
    "- [Sampler, Samplers, Everywhere...](http://mattpitkin.github.io/samplers-demo/pages/samplers-samplers-everywhere/) by Matt Pitkin.\n",
    "- [MCMC Interactive Demo](https://chi-feng.github.io/mcmc-demo/app.html) by Chi Feng.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "* [Practical MCMC](#one)\n",
    "* [MCMC Parameter Estimation In The Wild](#two)\n",
    "* [MCMC With Emcee](#three)\n",
    "* [MCMC With PyMC3](#four)\n",
    "* [MCMC With The PTMCMCSampler](#five)\n",
    "\n",
    "---\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In case you didn't do it last time:*** Please pause for a few minutes and install these packages `emcee`, `pymc3`, and `PTMCMCSampler` before going through today's notebook. Make sure this notebook is in the correct Python kernel for the class conda environment before executing each of the following cells in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emcee in c:\\users\\tonyt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\tonyt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from emcee) (2.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymc\n",
      "  Using cached pymc-5.20.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting arviz>=0.13.0 (from pymc)\n",
      "  Using cached arviz-0.20.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting cachetools>=4.2.1 (from pymc)\n",
      "  Using cached cachetools-5.5.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting cloudpickle (from pymc)\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: numpy>=1.25.0 in c:\\users\\tonyt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pymc) (2.2.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\tonyt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pymc) (2.2.3)\n",
      "INFO: pip is looking at multiple versions of pymc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pymc\n",
      "  Using cached pymc-5.20.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached pymc-5.19.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached pymc-5.19.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached pymc-5.18.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached pymc-5.18.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached pymc-5.18.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached pymc-5.17.0-py3-none-any.whl.metadata (15 kB)\n",
      "INFO: pip is still looking at multiple versions of pymc to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached pymc-5.16.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached pymc-5.16.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached pymc-5.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached pymc-5.15.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached pymc-5.15.0-py3-none-any.whl.metadata (10 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached pymc-5.14.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached pymc-5.13.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached pymc-5.13.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached pymc-5.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fastprogress>=0.2.0 (from pymc)\n",
      "  Using cached fastprogress-1.0.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pymc\n",
      "  Using cached pymc-5.11.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached pymc-5.10.4-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached pymc-5.10.3-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached pymc-5.10.2-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached pymc-5.10.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached pymc-5.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached pymc-5.9.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pytensor<2.18,>=2.17.0 (from pymc)\n",
      "  Using cached pytensor-2.17.1.tar.gz (3.5 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × pip subprocess to install build dependencies did not run successfully.\n",
      "  │ exit code: 2\n",
      "  ╰─> [121 lines of output]\n",
      "      Collecting setuptools>=48.0.0\n",
      "        Using cached setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "      Collecting cython\n",
      "        Using cached Cython-3.0.12-cp313-cp313-win_amd64.whl.metadata (3.6 kB)\n",
      "      Collecting numpy<1.26,>=1.17.0\n",
      "        Using cached numpy-1.25.2.tar.gz (10.8 MB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "      ERROR: Exception:\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 106, in _run_wrapper\n",
      "          status = _inner_run()\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 97, in _inner_run\n",
      "          return self.run(options, args)\n",
      "                 ~~~~~~~~^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
      "          return func(self, options, args)\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 386, in run\n",
      "          requirement_set = resolver.resolve(\n",
      "              reqs, check_supported_wheels=not options.target_dir\n",
      "          )\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 95, in resolve\n",
      "          result = self._result = resolver.resolve(\n",
      "                                  ~~~~~~~~~~~~~~~~^\n",
      "              collected.requirements, max_rounds=limit_how_complex_resolution_can_be\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "          )\n",
      "          ^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 546, in resolve\n",
      "          state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 397, in resolve\n",
      "          self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "          ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 173, in _add_to_criteria\n",
      "          if not criterion.candidates:\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 156, in __bool__\n",
      "          return bool(self._sequence)\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 174, in __bool__\n",
      "          return any(self)\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 162, in <genexpr>\n",
      "          return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "                             ^^^^^^^^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 53, in _iter_built\n",
      "          candidate = func()\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 187, in _make_candidate_from_link\n",
      "          base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n",
      "                                          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "              link, template, name, version\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "          )\n",
      "          ^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 233, in _make_base_candidate_from_link\n",
      "          self._link_candidate_cache[link] = LinkCandidate(\n",
      "                                             ~~~~~~~~~~~~~^\n",
      "              link,\n",
      "              ^^^^^\n",
      "          ...<3 lines>...\n",
      "              version=version,\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "          )\n",
      "          ^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 304, in __init__\n",
      "          super().__init__(\n",
      "          ~~~~~~~~~~~~~~~~^\n",
      "              link=link,\n",
      "              ^^^^^^^^^^\n",
      "          ...<4 lines>...\n",
      "              version=version,\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "          )\n",
      "          ^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 159, in __init__\n",
      "          self.dist = self._prepare()\n",
      "                      ~~~~~~~~~~~~~^^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 236, in _prepare\n",
      "          dist = self._prepare_distribution()\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 315, in _prepare_distribution\n",
      "          return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 527, in prepare_linked_requirement\n",
      "          return self._prepare_linked_requirement(req, parallel_builds)\n",
      "                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 642, in _prepare_linked_requirement\n",
      "          dist = _get_prepared_distribution(\n",
      "              req,\n",
      "          ...<3 lines>...\n",
      "              self.check_build_deps,\n",
      "          )\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 72, in _get_prepared_distribution\n",
      "          abstract_dist.prepare_distribution_metadata(\n",
      "          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "              finder, build_isolation, check_build_deps\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "          )\n",
      "          ^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 56, in prepare_distribution_metadata\n",
      "          self._install_build_reqs(finder)\n",
      "          ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 126, in _install_build_reqs\n",
      "          build_reqs = self._get_build_requires_wheel()\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 103, in _get_build_requires_wheel\n",
      "          return backend.get_requires_for_build_wheel()\n",
      "                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 702, in get_requires_for_build_wheel\n",
      "          return super().get_requires_for_build_wheel(config_settings=cs)\n",
      "                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 196, in get_requires_for_build_wheel\n",
      "          return self._call_hook(\n",
      "                 ~~~~~~~~~~~~~~~^\n",
      "              \"get_requires_for_build_wheel\", {\"config_settings\": config_settings}\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "          )\n",
      "          ^\n",
      "        File \"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 402, in _call_hook\n",
      "          raise BackendUnavailable(\n",
      "          ...<4 lines>...\n",
      "          )\n",
      "      pip._vendor.pyproject_hooks._impl.BackendUnavailable: Cannot import 'setuptools.build_meta'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× pip subprocess to install build dependencies did not run successfully.\n",
      "│ exit code: 2\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install pymc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/jellis18/PTMCMCSampler@master\n",
      "  Cloning https://github.com/jellis18/PTMCMCSampler (to revision master) to c:\\users\\tonyt\\appdata\\local\\temp\\pip-req-build-bn_tn0o3\n",
      "  Resolved https://github.com/jellis18/PTMCMCSampler to commit 5e3bebf0a6aab060483ba9d2227111f8e314fab5\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.16.3 in c:\\users\\tonyt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ptmcmcsampler==2.1.3.dev2+g5e3bebf) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\tonyt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ptmcmcsampler==2.1.3.dev2+g5e3bebf) (1.15.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/jellis18/PTMCMCSampler 'C:\\Users\\tonyt\\AppData\\Local\\Temp\\pip-req-build-bn_tn0o3'\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/jellis18/PTMCMCSampler@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/dfm/acor@master\n",
      "  Cloning https://github.com/dfm/acor (to revision master) to c:\\users\\tonyt\\appdata\\local\\temp\\pip-req-build-ijb4pinq\n",
      "  Resolved https://github.com/dfm/acor to commit b55eb8efa7df6c73b6f3f0c9b64fa1c801e8f821\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/dfm/acor 'C:\\Users\\tonyt\\AppData\\Local\\Temp\\pip-req-build-ijb4pinq'\n",
      "  Running command git checkout -b master --track origin/master\n",
      "  Branch 'master' set up to track remote branch 'master' from 'origin'.\n",
      "  Switched to a new branch 'master'\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [23 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \u001b[35m\"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "          json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                                   \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "          return hook(config_settings)\n",
      "        File \u001b[35m\"C:\\Users\\tonyt\\AppData\\Local\\Temp\\pip-build-env-cpxf8p52\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m334\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "          return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\tonyt\\AppData\\Local\\Temp\\pip-build-env-cpxf8p52\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m304\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "          \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\tonyt\\AppData\\Local\\Temp\\pip-build-env-cpxf8p52\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m522\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\tonyt\\AppData\\Local\\Temp\\pip-build-env-cpxf8p52\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m320\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m14\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'numpy'\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/dfm/acor@master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For installation issues with `pymc`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check what your python version is:\n",
    "\n",
    "```\n",
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "```\n",
    "\n",
    "- If you have a python version >3.13, then you will need to downgrade to a lower python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.0\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To downgrade the python version in your environment:\n",
    "\n",
    "- open your terminal\n",
    "- activate our class environment with `conda activate your_env_name`\n",
    "- do: `conda install python=3.12`\n",
    "\n",
    "**Note:** once you update your python, there might be times where you might need to reinstall some packages. You can do this with our usual way of installing packages (either via `pip` or `conda install`, whichever one you prefer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For installation issues with `acor`:\n",
    "\n",
    "If the `pip install` method above does not work:\n",
    "\n",
    "- Open your terminal.\n",
    "- Activate your class environment.\n",
    "- Clone the repository from https://github.com/dfm/acor somewhere on your machine (**not in your course directory**).\n",
    "- Enter the directory with `cd`.\n",
    "- Run `python setup.py install`.\n",
    "\n",
    "The final command will probably still result in what looks like an error in the installation process, but `acor` will be installed in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical MCMC <a class=\"anchor\" id=\"one\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tonyt\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\astroML\\linear_model\\linear_regression_errors.py:10: UserWarning: LinearRegressionwithErrors requires PyMC3 to be installed\n",
      "  warnings.warn('LinearRegressionwithErrors requires PyMC3 to be installed')\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from scipy import integrate\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import cauchy\n",
    "from astroML.plotting import hist\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical MCMC chain checks\n",
    "\n",
    "1. **CHECK ACCEPTANCE:** some MCMC samplers give an updating estimate of the current acceptance rate of new samples. Ideally for a sampler using some form of Metropolis-Hastings, this should be somewhere between $\\sim20-50\\%$ depending on the type of problem you're trying to solve.\n",
    "\n",
    "    - If the acceptance rate is high, the chain is moving but might not be exploring well. This gives high acceptance rate but poor global exploration of the posterior surface.\n",
    "    \n",
    "    - If the acceptance rate is low, the chain is hardly moving meaning that it's stuck in a rut or trying to jump to new points that are too far away.\n",
    "    \n",
    "\n",
    "2. **CHECK TRACEPLOTS:** After getting an idea of the acceptance rate, make traceplots of your chain. Ideally, our tracplot in each parameter would be mixing well (moving across parameter space without getting stuck), and carving out the same patch of parameter space on average. This will tell you whether your chain is getting stuck or encountering inefficiencies.\n",
    "\n",
    "\n",
    "3. **CHECK AUTOCORRELATION LENGTH:** The MCMC chain with Metropolis-Hastings will not give fully-independent random samples. The next point is influenced by where the previous point was. We need to check how much to down-sample the chain so that the points lack memory and influence from others. This is given by the ***autocorrelation length***. \n",
    "\n",
    "\n",
    "Look again at the plots below for an arbitrary problem (my own image). The 1st column is the trace, the 2nd is the histogram of the chain, and the 3rd column is the acceptance rate of newly proposed samples. Familiarize yourself with the kind of inspections needed for MCMC chains.\n",
    "\n",
    "- **In the top row, the proposal width was too small**. \n",
    "- **In the middle row, the proposal width was too big**. \n",
    "- **Only the bottom row shows reasonable sampling. This is the Goldilocks scenario.**\n",
    "\n",
    "![](./figures/fig_taylor_mcmc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next several cells will load up videos from `Vimeo` to demonstrate how Metropolis-Hastings--based MCMC samples (i) a square region, (ii) a diagonal region, (iii) a cross-diagonal region, and finally... (iv) a banana. Because if you can't sample from a banana then what's the use of your MCMC? Execute the cells and watch through them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://player.vimeo.com/video/19274900\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274900\">Metropolis in the Square</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://player.vimeo.com/video/19274900\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274900\">Metropolis in the Square</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://player.vimeo.com/video/19274173\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274173\">Metropolis in Diagonal Region</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe src=\"https://player.vimeo.com/video/19274173\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274173\">Metropolis in Diagonal Region</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://player.vimeo.com/video/19275365\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274173\">Metropolis in Diagonal Region</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe src=\"https://player.vimeo.com/video/19275365\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274173\">Metropolis in Diagonal Region</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://player.vimeo.com/video/22616409\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274173\">Metropolis in Diagonal Region</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe src=\"https://player.vimeo.com/video/22616409\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274173\">Metropolis in Diagonal Region</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizing sampling\n",
    "\n",
    "There are several ways of improving the way we propose new sample locations in Metropolis-Hastings. Here are a few.\n",
    "\n",
    "#### (a) Adaptive Metropolis (AM)\n",
    "\n",
    "In AM you use the **empirically-estimated parameter covariance matrix to tune the width of the Gaussian proposal  distribution**. Tuning is updated during the sampling in order to reach optimal mixing. In practice this  means  that  one  uses  the  entire  past  history  of  the  chain  up  until  the  current point to estimate the parameter covariance matrix, scaling this covariance matrix by $\\alpha= 2.38^2/N_\\mathrm{param}$ to reach the optimal $\\sim25\\%$ proposal acceptance rate. \n",
    "\n",
    "Practically speaking, the procedure is\n",
    "- Estimate the $N_\\mathrm{param}\\times N_\\mathrm{param}$ parameter covariance matrix, $C$, using all samples. Standard numpy or scipy algorithms can do this. \n",
    "- Factorize the matrix using a Cholesky algorithm, such that $C = L L^T$.\n",
    "- Draw a new proposed point such that $y = x_i + \\sqrt{\\alpha} Lu$, where $x_i$ is the current point, and $u$ is an $N_\\mathrm{param}$-dimensional vector of random draws from a zero-mean unit-variance Gaussian.\n",
    "\n",
    "*One subtlety here is that by using more than just the most recent point to tune the sampling, our chain is no longer Markovian. This is easily resolved by allowing the chain to pass through a proposal tuning stage using AM, after which the proposal covariance matrix is frozen so that the chain is Markovian then on.*\n",
    "\n",
    "#### (b) Single Component Adaptive Metropolis (SCAM)\n",
    "\n",
    "With high-dimensional model parameter spaces, or even target posterior distributions with significant covariances amongst some parameters, the AM method may suffer from low acceptance rates. One method that addresses this is a variant on AM called Single Component Adaptive Metropolis (SCAM).  **This  involves  jumping  along selected eigenvectors (or principal axes) of the parameter covariance matrix**, which is equivalent to jumping in only one uncorrelated parameter at a time. (We'll see more of principal axes later in the course)\n",
    "\n",
    "- We take our parameter covariance matrix as in AM, but this time work out the eigenvalues and eigenvectors, $C = D\\Lambda D^T$, where $D$ is a unitary matrix with eigenvectors as columns, and $\\Lambda = \\mathrm{diag}(\\sigma^2_\\Lambda$) is a diagonal matrix of eigenvalues. \n",
    "- A SCAM jump corresponds to a zero-mean unit-variance jump in a randomly chosen uncorrelated parameter, equivalent to jumping along one of the eigenvectors. \n",
    "- A proposal draw is given by $y = x_i+ 2.4 D_j u_j$, where $D_j$ is a randomly chosen column of D corresponding to the $j$th eigenvector of $C$, and $u_j \\sim \\mathcal{N}(0,\\sigma^j_\\Lambda)$.\n",
    "\n",
    "#### (c) Differential Evolution (DE)\n",
    "\n",
    "Another popular proposal scheme is DE, which is a simple *genetic algorithm* that treats the past history of the  chain up until the current point as a population. \n",
    "\n",
    "- In DE, you choose two random points from the chain’s history to construct a difference vector along which the chain can jump. \n",
    "- A DE proposal draw is given by $y = x_i + \\beta(x_{r1} − x_{r2})$, where $x_{r1,2}$ are parameter vectors from two randomly chosen points in the past history of the chain, and $\\beta$ is a scaling factor that is usually set to be the same as the AM scaling factor, i.e., $2.38 / \\sqrt{N_\\mathrm{param}}$.\n",
    "\n",
    "\n",
    "#### The Full Proposal Cocktail\n",
    "\n",
    "Real world MCMC should use a cocktail of proposal schemes, aimed at ensuring convergence to the target posterior distribution with **minimal burn-in**, **optimal acceptance rate**, and as **short an autocorrelation length** as possible. \n",
    "\n",
    "At each MCMC iteration the proposed parameter location can be drawn according to a weighted list of schemes, involving **(i) AM, (ii) SCAM, (iii) DE, (iv) empirical proposal distributions (e.g. from previous analyses), and finally (iv) draws from the parameter prior distribution**. \n",
    "\n",
    "The final prior-draw scheme allows for occasional large departures from regions of high likelihood, ensuring that we are exploring the full parameter landscape well, and avoiding the possibility of getting stuck in local maxima. \n",
    "\n",
    "Really, you can use any reasonable distribution you like to propose points. Your only constraint is to ensure that detailed balance is maintained through the relevant transition weightings in  the  Metropolis-Hastings ratio, $p_\\mathrm{acc}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MCMC Parameter Estimation In The Wild <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "Let's look at some more involved examples. We will use two popular python modules: `emcee` and `PyMC`. I'll also show you a great but less used sampler that is the standard one for my field of PTA gravitational-wave astrophysics: `PTMCMCSampler`. Quoting Jake VanderPlas for `emcee` and `PyMC`: \n",
    "[Jake's blog:](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/)\n",
    "\n",
    "### emcee\n",
    "\n",
    "> The emcee package (*also known as MCMC Hammer, which is in the running for best Python package name in history*) is a Pure Python package written by Astronomer Dan Foreman-Mackey. It is a lightweight package which implements a fairly sophisticated Affine-invariant Hamiltonian MCMC. Because the package is pure Python (i.e. it contains no compiled extensions) it is extremely easy to install; with pip, simply type at the command-line \"pip install emcee\".\n",
    "\n",
    "Emcee does not have much specific boilerplate code; it simply requires you to pass it a Python function which returns a value proportional to the log-posterior probability, and returns samples from that posterior.*\n",
    "\n",
    "### PyMC\n",
    "\n",
    "> The PyMC package has many more features than emcee, including built-in support for efficient sampling of common prior distributions. PyMC by default uses the classic Metropolis-Hastings sampler, one of the earliest MCMC algorithms. For performance, it uses compiled fortran libraries, so it is less trivial to install using tools like pip. PyMC binaries for many systems can be quite easily installed with conda.*\n",
    "\n",
    "More details about PyMC are available from [the pyMC User Guide](https://pymc-devs.github.io/pymc/), but note that we are going to be using [PyMC3](https://docs.pymc.io/).\n",
    "\n",
    "### PTMCMCSampler\n",
    "\n",
    "> This is a bare-bones sampler, requiring only that the user provide a log-likelihood function and a log-prior function. The user can change the relative amounts of AM, SCAM, and DE being used to propose new points to jump to. The great thing about this sampler is that one can add new custom proposal schemes according to the user's preference. \n",
    "\n",
    "> Also, this sampler implements **parallel tempering MCMC (PTMCMC)**. We won't go into detail about this, but suffice it to say that multiple copies of the MCMC are run in parallel, where the copies actually sample from different roots of the likelihood. The higher roots we take, the more the likelihood is flattened out, making it easier to sample. These \"rooted\" chains then communicate back to the main chain, improving exploration of the parameter space. All these chains can actually be post-processed to get the Bayesian evidence too, in a scheme called **thermodynamic integration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MCMC with emcee <a class=\"anchor\" id=\"three\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `emcee` and generate some homoescedastic Gaussian data. We'll assume the standard deviation is know, so we're only estimating $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "np.random.seed(21)\n",
    "Ndata = 100\n",
    "mu = 1.0\n",
    "sigma = 0.5 # assumed known \n",
    "data = stats.norm(mu, sigma).rvs(Ndata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all the relevant functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Likelihood(x, sigma, data):\n",
    "    # Gaussian likelihood \n",
    "    return np.prod(np.exp(-(data-x)**2 /2 /sigma**2))\n",
    "\n",
    "def Prior(x):\n",
    "    return 1.0 / 10   # flat: it cancels out and has no effect \n",
    "\n",
    "def myPosterior(x, sigma, data):\n",
    "    return Likelihood(x, sigma, data) * Prior(x)\n",
    "\n",
    "# emcee wants ln of posterior pdf\n",
    "def myLogPosterior(x, sigma, data):\n",
    "    return np.log(myPosterior(x, sigma, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`emcee` combines multiple \"walkers\", each of which is its own MCMC chain. The number of trace results will be nwalkers $\\times$ nsteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 1  # number of parameters in the model\n",
    "nwalkers = 6  # number of MCMC walkers\n",
    "burn = 1000  # \"burn-in\" period to let chains stabilize\n",
    "nsteps = 5000  # number of MCMC steps to take **for each walker**\n",
    "\n",
    "# initialize theta \n",
    "np.random.seed(0)\n",
    "starting_guesses = np.random.random((nwalkers, ndim))\n",
    "\n",
    "# the function call where all the work happens: \n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, myLogPosterior, args=[sigma, data])\n",
    "sampler.run_mcmc(starting_guesses, nsteps)\n",
    " \n",
    "# sampler.chain is of shape (nwalkers, nsteps, ndim)\n",
    "# throw-out the burn-in points and reshape:\n",
    "emcee_trace  = sampler.chain[:, burn:, :].reshape(-1, ndim)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampler.chain.shape) #original chain structure\n",
    "print(emcee_trace.shape) #burned and flattened chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "fig.subplots_adjust(left=0.11, right=0.95, \n",
    "                    wspace=0.35, bottom=0.18)\n",
    "\n",
    "chainE = emcee_trace #[0]\n",
    "M = np.size(chainE)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "xgrid = np.linspace(1, M, M)\n",
    "plt.plot(xgrid, chainE)\n",
    "ax1.axis([0, M, np.min(chainE), 1.1*np.max(chainE)])\n",
    "plt.xlabel('number')\n",
    "plt.ylabel('chain')\n",
    "\n",
    "# plot running mean: \n",
    "meanC = [np.mean(chainE[:int(N)]) for N in xgrid]\n",
    "ax1.plot(xgrid, meanC, c='red', label='chain mean') \n",
    "ax1.plot(xgrid, 0*xgrid + np.mean(data),\n",
    "         c='yellow',label='data mean')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "# skip first burn samples\n",
    "Nburn = 1000\n",
    "Nchain = np.size(chainE[xgrid>burn])\n",
    "Nhist, bins, patches = plt.hist(chainE[xgrid>Nburn], \n",
    "                                bins='auto', histtype='stepfilled')\n",
    "\n",
    "# plot expectations based on central limit theorem\n",
    "binwidth = bins[1] - bins[0]\n",
    "muCLT = np.mean(data)\n",
    "sigCLT = np.std(data)/np.sqrt(Ndata)\n",
    "muGrid = np.linspace(0.7, 1.3, 500)\n",
    "gauss = Nchain * binwidth * stats.norm(muCLT, sigCLT).pdf(muGrid) \n",
    "ax2.plot(muGrid, gauss, c='red') \n",
    "\n",
    "ax2.set_ylabel('p(chain)')\n",
    "ax2.set_xlabel('chain values')\n",
    "ax2.set_xlim(0.7, 1.3)\n",
    "ax2.set_ylim(0, 1.2*np.max(gauss))\n",
    "ax2.set_title(r'Chain from emcee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MCMC with PyMC3 <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "Now we will use pyMC3 to get a 2-dimensional posterior pdf for location and scale parameters using a sample drawn from a **Cauchy distribution**. Remember a Cauchy distribution formally has no mean or standard deviation because its tails fall off shallower than $1/x^2$.\n",
    "\n",
    "The following is code adapted from Figure 5.22 of the textbook. Initially, we load in `PyMC3` and define the Cauchy log likelihood. \n",
    "\n",
    "***NOTE:*** The code here for the Cauchy likelihood is actually only for when we look at the analytic estimate, since `PyMC3` has ready-made modules for many standard functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import cauchy\n",
    "import pymc as pm\n",
    "\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "def cauchy_logL(xi, sigma, mu):\n",
    "    \"\"\"Equation 5.74: cauchy likelihood\"\"\"\n",
    "    xi = np.asarray(xi)\n",
    "    n = xi.size\n",
    "    shape = np.broadcast(sigma, mu).shape\n",
    "\n",
    "    xi = xi.reshape(xi.shape + tuple([1 for s in shape]))\n",
    "\n",
    "    return ((n - 1) * np.log(sigma)\n",
    "            - np.sum(np.log(sigma ** 2 + (xi - mu) ** 2), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data from the Cauchy distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Draw the sample from a Cauchy distribution\n",
    "np.random.seed(44)\n",
    "mu_0 = 0\n",
    "gamma_0 = 2\n",
    "xi = cauchy(mu_0, gamma_0).rvs(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the `PyMC3` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Set up and run MCMC:\n",
    "with pm.Model():\n",
    "    mu = pm.Uniform('mu', -5, 5) #uniform in Cauchy mu\n",
    "    log_gamma = pm.Uniform('log_gamma', -10, 10) #uniform in log of Cauchy gamma\n",
    "\n",
    "    # set up our observed variable x\n",
    "    # i.e. read this as 'x is distributed as a Cauchy variable'\n",
    "    x = pm.Cauchy('x', mu, np.exp(log_gamma), observed=xi)\n",
    "\n",
    "    trace = pm.sample(draws=12000, tune=1000, cores=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have seen some output messages along the lines of \"Auto-assigning NUTS sampler...\". `PyMC3` is very sophisticated, and will automatically decide for you the best tools for the job. \n",
    "\n",
    "In this case, it decided you needed **No-U-Turn Hamiltonian gradient-based sampling**, where gradient information about the Cauchy likelihood helped in the exploration of parameter space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(trace.posterior['mu']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute histogram of results to plot below\n",
    "L_MCMC, mu_bins, gamma_bins = np.histogram2d(np.array(trace.posterior['mu']).flatten(),\n",
    "                                             np.exp(np.array(trace.posterior['log_gamma'])).flatten(),\n",
    "                                             bins=(np.linspace(-5, 5, 41),\n",
    "                                                   np.linspace(0, 5, 41)))\n",
    "L_MCMC[L_MCMC == 0] = 1E-16  # prevents zero-division errors\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Compute likelihood analytically for comparison\n",
    "mu = np.linspace(-5, 5, 70)\n",
    "gamma = np.linspace(0.1, 5, 70)\n",
    "logL = cauchy_logL(xi, gamma[:, np.newaxis], mu)\n",
    "logL -= logL.max()\n",
    "\n",
    "p_mu = np.exp(logL).sum(0)\n",
    "p_mu /= p_mu.sum() * (mu[1] - mu[0])\n",
    "\n",
    "p_gamma = np.exp(logL).sum(1)\n",
    "p_gamma /= p_gamma.sum() * (gamma[1] - gamma[0])\n",
    "\n",
    "hist_mu, bins_mu = np.histogram(np.array(trace.posterior['mu']).flatten(), \n",
    "                                bins=mu_bins, density=True)\n",
    "hist_gamma, bins_gamma = np.histogram(np.exp(np.array(trace.posterior['log_gamma'])).flatten(),\n",
    "                                      bins=gamma_bins, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# plot all the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# first axis: likelihood contours\n",
    "ax1 = fig.add_axes((0.4, 0.4, 0.55, 0.55))\n",
    "ax1.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax1.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax1.contour(mu, gamma, convert_to_stdev(logL),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='b', linestyles='dashed')\n",
    "\n",
    "ax1.contour(0.5 * (mu_bins[:-1] + mu_bins[1:]),\n",
    "            0.5 * (gamma_bins[:-1] + gamma_bins[1:]),\n",
    "            convert_to_stdev(np.log(L_MCMC.T)),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='k')\n",
    "\n",
    "# second axis: marginalized over mu\n",
    "ax2 = fig.add_axes((0.1, 0.4, 0.29, 0.55))\n",
    "ax2.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax2.plot(hist_gamma, 0.5 * (bins_gamma[1:] + bins_gamma[:-1]\n",
    "                            - bins_gamma[1] + bins_gamma[0]),\n",
    "         '-k', drawstyle='steps')\n",
    "ax2.plot(p_gamma, gamma, '--b')\n",
    "ax2.set_ylabel(r'$\\gamma$')\n",
    "ax2.set_ylim(0, 5)\n",
    "\n",
    "# third axis: marginalized over gamma\n",
    "ax3 = fig.add_axes((0.4, 0.1, 0.55, 0.29))\n",
    "ax3.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax3.plot(0.5 * (bins_mu[1:] + bins_mu[:-1]), hist_mu,\n",
    "         '-k', drawstyle='steps-mid')\n",
    "ax3.plot(mu, p_mu, '--b')\n",
    "ax3.set_xlabel(r'$\\mu$')\n",
    "plt.xlim(-5, 5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting $2$D joint posterior pdf corner plot shows analytic results as blue dashed lines, and MCMC sampling results as black contours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC With The PTMCMCSampler <a class=\"anchor\" id=\"five\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore the more bare-bones sampler. This helps to expose a lot of what is going on in the other samplers, where it's difficult to see what happens under the hood.\n",
    "\n",
    "**We're going to analyze a homoescedastic Gaussian dataset, but this time search over the mean and standard deviation with our MCMC.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't worry about any warning messages regarding mpi4py\n",
    "from PTMCMCSampler.PTMCMCSampler import PTSampler as ptmcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(21)\n",
    "Ndata = 100\n",
    "mu = 1.0\n",
    "sigma = 0.5 # assumed known \n",
    "data = stats.norm(mu, sigma).rvs(Ndata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function below to create a prior function that is the multiplication of a uniform prior for the mean over $0.5$ to $1.5$, and a uniform prior for the standard deviation between $0.1$ and $0.9$. \n",
    "\n",
    "The priors are separable and independent, so the overall prior should be the product of the prior for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Likelihood(x, data):\n",
    "    # Gaussian likelihood \n",
    "    return np.prod(stats.norm(loc=x[0], scale=x[1]).pdf(data))\n",
    "\n",
    "def Prior(x):\n",
    "    # create a uniform prior in mu and sigma\n",
    "    return ___\n",
    "\n",
    "def logLikelihood(x):\n",
    "    return np.log(Likelihood(x, data))\n",
    "\n",
    "def logPrior(x):\n",
    "    return np.log(Prior(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of parameter space\n",
    "ndim = 2\n",
    "\n",
    "# initial jump covariance matrix\n",
    "cov = np.diag(np.ones(ndim) * 0.01**2)\n",
    "\n",
    "# intialize sampler\n",
    "sampler = ptmcmc(ndim, logLikelihood, logPrior, cov, \n",
    "                 outDir='./my_ptmcmc_chain', resume=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler for N steps\n",
    "N = int(3e4)\n",
    "x0 = np.array([0.9, 0.3])\n",
    "sampler.sample(x0, N, SCAMweight=30, AMweight=15, DEweight=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the chain\n",
    "chain = np.loadtxt('./my_ptmcmc_chain/chain_1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the final 4 columns in your chain array. Those are chain diagnostics, not parameters.\n",
    "\n",
    "<font color='red'>OK, now that you have your chain, it's up to you to diagnose it.</font>\n",
    "    \n",
    "- Make traceplots in both $\\mu$ and $\\sigma$. \n",
    "- Discuss when you should cut off burn-in. \n",
    "- Compute the autocorrelation length of the chain, and down-sample it to select only points every autocorrelation length. \n",
    "- Finally, make a corner plot that has labels, shows titles, and has $68\\%$ and $95\\%$ quantiles and levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acor\n",
    "acor.acor(chain[:,0]) \n",
    "# the first element of the tuple is the length between independent samples.\n",
    "# thin by the nearest integer."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
